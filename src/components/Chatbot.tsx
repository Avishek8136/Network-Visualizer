import React, { useState, useRef, useEffect } from 'react';
import { Send, Bot, User, MessageSquare, X, Minimize2, Maximize2 } from 'lucide-react';
import type { ChatMessage } from '../types';

interface ChatbotProps {
  currentModel?: string;
}

const Chatbot: React.FC<ChatbotProps> = ({ currentModel = 'Neural Networks' }) => {
  const [messages, setMessages] = useState<ChatMessage[]>([
    {
      id: '1',
      role: 'assistant',
      content: `Hello! üëã I'm your Neural Network Assistant with comprehensive knowledge about complete CNN architectures!

üí° **Updated Knowledge Base** - Now with COMPLETE architectures:
‚Ä¢ **AlexNet (2012)**: All 8 layers (5 conv + 3 FC) = 12 total steps
‚Ä¢ **VGG-16 (2014)**: All 16 layers (13 conv + 3 FC) = 22 total steps  
‚Ä¢ **GoogLeNet (2014)**: All 22 layers with 9 inception modules
‚Ä¢ **ResNet-50 (2015)**: All 50 layers with residual blocks

ÔøΩ **Ask me about**:
‚Ä¢ Layer-by-layer breakdowns: "Show me all VGG-16 layers"
‚Ä¢ Innovations: "Explain skip connections", "What are inception modules?"
‚Ä¢ Comparisons: "Compare AlexNet vs ResNet"
‚Ä¢ Technical details: "How many parameters in FC layers?"
‚Ä¢ Concepts: "What is bottleneck design?"

**How I work**: Pattern matching with curated knowledge base covering 50+ topics!`,
      timestamp: new Date(),
    },
  ]);
  const [input, setInput] = useState('');
  const [isMinimized, setIsMinimized] = useState(false);
  const [isExpanded, setIsExpanded] = useState(false);
  const messagesEndRef = useRef<HTMLDivElement>(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  const getAIResponse = (userMessage: string): string => {
    const lowerMessage = userMessage.toLowerCase();
    
    // Inference Explorer specific questions
    if (lowerMessage.includes('inference') || lowerMessage.includes('layer by layer')) {
      return "üéØ **Inference Explorer** shows real neural network processing!\n\nEach layer transforms the image:\n‚Ä¢ **Conv layers**: Extract features (edges‚Üítextures‚Üípatterns)\n‚Ä¢ **Pooling**: Reduces size, keeps important info\n‚Ä¢ **Batch Norm**: Stabilizes training\n‚Ä¢ **Activation (ReLU)**: Adds non-linearity (max(0,x))\n‚Ä¢ **Global Avg Pool**: Converts feature maps to vectors\n‚Ä¢ **Dense**: Learns class patterns\n‚Ä¢ **Softmax**: Converts to probabilities\n\nWatch how spatial dimensions shrink but channels increase as the network extracts deeper features!";
    }
    
    if (lowerMessage.includes('feature map') || lowerMessage.includes('activation')) {
      return "üîç **Feature Maps** are the outputs of convolutional layers!\n\nEach filter in a conv layer produces one feature map:\n‚Ä¢ **Early layers**: Detect edges, colors, simple patterns\n‚Ä¢ **Middle layers**: Detect textures, shapes, parts\n‚Ä¢ **Deep layers**: Detect complex objects, faces, scenes\n\nVisualization shows first 16 channels. Bright areas = strong activation = important features detected!";
    }
    
    if (lowerMessage.includes('weight') || lowerMessage.includes('pretrained')) {
      return "‚öñÔ∏è **Pretrained Weights** are learned from ImageNet (1.2M images, 1000 classes)!\n\nEach layer has weights:\n‚Ä¢ **Conv filters**: 3D tensors (height√ówidth√óchannels√ófilters)\n‚Ä¢ **Batch Norm**: Scale & shift parameters\n‚Ä¢ **Dense layers**: 2D matrices (input√óoutput)\n\nMobileNet was trained for weeks on powerful GPUs. You're using those learned patterns instantly in your browser!";
    }
    
    if (lowerMessage.includes('how image change') || lowerMessage.includes('transformation')) {
      return "üñºÔ∏è **Image Transformation Through Layers**:\n\n1. **Input (224√ó224√ó3)**: RGB pixels\n2. **Conv1 (112√ó112√ó32)**: 32 edge detectors, spatial size halved\n3. **Pool (56√ó56√ó32)**: Downsampled, keeps important features\n4. **Conv2 (56√ó56√ó64)**: 64 texture detectors\n5. **Conv3 (28√ó28√ó128)**: 128 pattern detectors\n6. **Global Pool (1√ó1√ó128)**: Each feature map ‚Üí single value\n7. **Dense (256)**: High-level feature combinations\n8. **Output (1000)**: Probabilities for each class\n\nSpatial info: 224‚Üí112‚Üí56‚Üí28‚Üí14‚Üí1\nChannels: 3‚Üí32‚Üí64‚Üí128‚Üí256‚Üí1000";
    }
    
    if (lowerMessage.includes('why batch norm') || lowerMessage.includes('normalization')) {
      return "üìä **Batch Normalization** is crucial for deep networks!\n\n**Why use it?**\n‚Ä¢ Normalizes activations: mean=0, std=1\n‚Ä¢ Faster training (higher learning rates)\n‚Ä¢ Reduces internal covariate shift\n‚Ä¢ Acts as regularization\n‚Ä¢ Stabilizes gradients\n\n**How it works:**\n1. Normalize: (x - mean) / sqrt(variance + Œµ)\n2. Scale: Œ≥ * normalized_x\n3. Shift: + Œ≤\n\nŒ≥ and Œ≤ are learned parameters!";
    }
    
    if (lowerMessage.includes('1d array') || lowerMessage.includes('flatten') || lowerMessage.includes('vector')) {
      return "üìê **1D Vectors in Neural Networks**:\n\n**How 2D‚Üí1D happens:**\n‚Ä¢ **Global Avg Pooling**: Average each feature map (14√ó14‚Üí1)\n‚Ä¢ **Flatten**: Reshape (7√ó7√ó512 ‚Üí 25,088)\n‚Ä¢ Result: 1D vector ready for Dense layers\n\n**Why needed?**\nDense layers need fixed-size 1D input. They can't handle 2D spatial data directly.\n\n**Example:**\n3√ó3√ó2 feature maps ‚Üí [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18] (18 values)\n\nEach value represents activation strength for that spatial position & channel!";
    }
    
    if (lowerMessage.includes('why pooling') || lowerMessage.includes('maxpool')) {
      return "üåä **Pooling Layers** reduce dimensions intelligently!\n\n**Why use pooling?**\n‚Ä¢ Reduces computation (fewer parameters)\n‚Ä¢ Provides translation invariance\n‚Ä¢ Extracts dominant features\n‚Ä¢ Prevents overfitting\n‚Ä¢ Increases receptive field\n\n**Max Pooling (2√ó2):**\n```\n[1 2]     \n[3 4] ‚Üí 4 (takes maximum)\n```\n\n**Effect:** 224√ó224 ‚Üí 112√ó112 (75% fewer pixels!)\n\nNo learnable parameters, just downsampling!";
    }
    
    // Knowledge base for common questions - UPDATED WITH COMPLETE ARCHITECTURES
    if (lowerMessage.includes('alexnet')) {
      return "üèÜ **AlexNet (2012)** - The Deep Learning Revolution!\n\n**Complete Architecture (8 layers):**\n‚Ä¢ **Conv1**: 96 filters, 11√ó11, stride 4 (227√ó227‚Üí55√ó55)\n‚Ä¢ **MaxPool1**: 3√ó3, stride 2 ‚Üí 27√ó27\n‚Ä¢ **Conv2**: 256 filters, 5√ó5 ‚Üí 27√ó27\n‚Ä¢ **MaxPool2**: 3√ó3, stride 2 ‚Üí 13√ó13\n‚Ä¢ **Conv3**: 384 filters, 3√ó3 ‚Üí 13√ó13\n‚Ä¢ **Conv4**: 384 filters, 3√ó3 ‚Üí 13√ó13\n‚Ä¢ **Conv5**: 256 filters, 3√ó3 ‚Üí 13√ó13\n‚Ä¢ **MaxPool3**: 3√ó3, stride 2 ‚Üí 6√ó6\n‚Ä¢ **FC6**: 4,096 units (102M params!)\n‚Ä¢ **FC7**: 4,096 units (16.8M params)\n‚Ä¢ **FC8**: 1,000 classes (4.1M params)\n\n**Innovations**: ReLU, dropout (50%), overlapping pooling, GPU training\n**Total**: ~60M parameters | Won ImageNet 2012 with 15.3% top-5 error\n\nüí° Use Inference Explorer to see all layers with real data!";
    }
    
    if (lowerMessage.includes('resnet')) {
      return "üîÑ **ResNet-50 (2015)** - Skip Connections Breakthrough!\n\n**Complete Architecture (50 layers):**\n‚Ä¢ **Conv1**: 64 filters, 7√ó7, stride 2 (224‚Üí112)\n‚Ä¢ **MaxPool**: 3√ó3, stride 2 (112‚Üí56)\n‚Ä¢ **Conv2_x**: 3 bottleneck blocks @56√ó56 (64‚Üí64‚Üí256)\n‚Ä¢ **Conv3_x**: 4 bottleneck blocks @28√ó28 (128‚Üí128‚Üí512)\n‚Ä¢ **Conv4_x**: 6 bottleneck blocks @14√ó14 (256‚Üí256‚Üí1024) ‚≠ê Deepest\n‚Ä¢ **Conv5_x**: 3 bottleneck blocks @7√ó7 (512‚Üí512‚Üí2048)\n‚Ä¢ **Global Avg Pool**: 7√ó7‚Üí1 (NO parameters!)\n‚Ä¢ **FC**: 2048‚Üí1,000 classes\n\n**Key Innovation**: y = F(x) + x (skip connections)\n‚Ä¢ Solves vanishing gradients\n‚Ä¢ Enables 50-152 layer networks\n‚Ä¢ Bottleneck blocks: 1√ó1 reduce ‚Üí 3√ó3 process ‚Üí 1√ó1 expand\n\n**Total**: ~25M params (5√ó less than VGG!) | Won ImageNet 2015 with 3.57% error\n\nüí° Inference Explorer shows residual blocks with real skip connections!";
    }
    
    if (lowerMessage.includes('googlenet') || lowerMessage.includes('inception')) {
      return "üéØ **GoogLeNet/Inception V1 (2014)** - Multi-Scale Efficiency!\n\n**Complete Architecture (22 layers, 9 inception modules):**\n‚Ä¢ **Conv1**: 64 filters, 7√ó7, stride 2 (224‚Üí112)\n‚Ä¢ **MaxPool1**: 3√ó3, stride 2 (112‚Üí56)\n‚Ä¢ **Conv2**: 192 filters, 3√ó3 ‚Üí 56√ó56\n‚Ä¢ **MaxPool2**: 3√ó3, stride 2 (56‚Üí28)\n‚Ä¢ **Inception 3a & 3b** @28√ó28 (2 modules)\n‚Ä¢ **MaxPool3** (28‚Üí14)\n‚Ä¢ **Inception 4a-4e** @14√ó14 (5 modules) ‚≠ê Most modules\n‚Ä¢ **MaxPool4** (14‚Üí7)\n‚Ä¢ **Inception 5a & 5b** @7√ó7 (2 modules)\n‚Ä¢ **Global Avg Pool** + Dropout (40%)\n‚Ä¢ **FC**: 1,024‚Üí1,000 classes\n\n**Inception Module** (4 parallel paths):\n1. 1√ó1 conv (point-wise features)\n2. 1√ó1‚Üí3√ó3 conv (local features)\n3. 1√ó1‚Üí5√ó5 conv (broader features)\n4. 3√ó3 pool‚Üí1√ó1 (max features)\n‚Üí ALL CONCATENATED!\n\n**Innovation**: 1√ó1 \"reduce\" convs save massive computation!\n**Total**: Only 7M params (20√ó less than VGG!) | Won ImageNet 2014 with 6.67% error\n\nüí° Inference Explorer shows inception modules with parallel paths!";
    }
    
    if (lowerMessage.includes('vgg')) {
      return "üìö **VGG-16 (2014)** - Depth Matters!\n\n**Complete Architecture (16 layers, 5 blocks):**\n**Block 1** (224‚Üí112): Conv1_1, Conv1_2 (64 filters, 3√ó3) + MaxPool\n**Block 2** (112‚Üí56): Conv2_1, Conv2_2 (128 filters, 3√ó3) + MaxPool\n**Block 3** (56‚Üí28): Conv3_1, Conv3_2, Conv3_3 (256 filters, 3√ó3) + MaxPool\n**Block 4** (28‚Üí14): Conv4_1, Conv4_2, Conv4_3 (512 filters, 3√ó3) + MaxPool\n**Block 5** (14‚Üí7): Conv5_1, Conv5_2, Conv5_3 (512 filters, 3√ó3) + MaxPool\n‚Ä¢ **FC6**: 25,088‚Üí4,096 (102M params! üò±)\n‚Ä¢ **FC7**: 4,096‚Üí4,096 (16.8M params)\n‚Ä¢ **FC8**: 4,096‚Üí1,000 classes\n\n**Architecture Pattern**: 2-2-3-3-3 convs per block\n**Filter Progression**: 64‚Üí128‚Üí256‚Üí512‚Üí512 (doubles each stage)\n\n**Key Innovation**: Stacked 3√ó3 convs are better than large kernels!\n‚Ä¢ Two 3√ó3 = same field as 5√ó5, but fewer params\n‚Ä¢ Three 3√ó3 = same field as 7√ó7\n\n**Total**: 138M params (123M in FC layers!) | Proved depth is crucial\n\nüí° Inference Explorer shows ALL 16 layers + 5 pooling + 3 FC = 22 total steps!";
    }
    
    if (lowerMessage.includes('convolution') || lowerMessage.includes('conv')) {
      return "Convolutional layers apply filters (kernels) to extract features from images. They use parameter sharing and local connectivity, making them efficient for spatial data. Parameters include: kernel size (e.g., 3√ó3), stride (step size), padding (border handling), and number of filters (output channels).";
    }
    
    if (lowerMessage.includes('pooling')) {
      return "Pooling layers reduce spatial dimensions while retaining important features. Max pooling takes the maximum value in each region, while average pooling takes the mean. This provides translation invariance and reduces computation. Common size: 2√ó2 with stride 2 (halves dimensions).";
    }
    
    if (lowerMessage.includes('relu') || lowerMessage.includes('activation')) {
      return "ReLU (Rectified Linear Unit) is f(x) = max(0, x). It's the most popular activation because it: 1) Prevents vanishing gradients, 2) Is computationally efficient, 3) Enables sparse activations. Other activations: Sigmoid, Tanh, Leaky ReLU, GELU. Output layers typically use Softmax for classification.";
    }
    
    if (lowerMessage.includes('dropout')) {
      return "Dropout randomly deactivates neurons during training (e.g., 50% dropout rate) to prevent overfitting. It forces the network to learn robust features that don't rely on specific neurons. At inference, all neurons are active but outputs are scaled. It's like training an ensemble of networks!";
    }
    
    if (lowerMessage.includes('batch norm')) {
      return "Batch Normalization normalizes layer inputs across mini-batches, which: 1) Accelerates training, 2) Allows higher learning rates, 3) Reduces sensitivity to initialization, 4) Acts as regularization. It normalizes to mean 0 and variance 1, then applies learned scale and shift parameters.";
    }
    
    if (lowerMessage.includes('transfer learning')) {
      return "Transfer learning uses pre-trained models (like AlexNet, ResNet on ImageNet) for new tasks. You can: 1) Use as feature extractor (freeze weights, train new classifier), 2) Fine-tune (unfreeze some layers, train with small learning rate). This works because early layers learn general features (edges, textures).";
    }
    
    if (lowerMessage.includes('overfitting')) {
      return "Overfitting occurs when a model learns training data too well, including noise. Prevention techniques: 1) Dropout, 2) Data augmentation, 3) L1/L2 regularization, 4) Early stopping, 5) Reduce model complexity, 6) More training data. Monitor validation loss - if it increases while training loss decreases, you're overfitting!";
    }
    
    if (lowerMessage.includes('imagenet')) {
      return "ImageNet is a large-scale dataset with 1.2M training images across 1000 classes. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) drove CNN innovation from 2010-2017. Top-5 error went from 28% (2010) to 2.25% (2017), surpassing human performance (~5%).";
    }
    
    if (lowerMessage.includes('parameter') || lowerMessage.includes('weight')) {
      return "Parameters are learnable values (weights and biases) that the network optimizes during training. Conv layer params = (kernel_height √ó kernel_width √ó input_channels + 1) √ó output_channels. FC layer params = (input_size + 1) √ó output_size. More parameters = more capacity but also more risk of overfitting.";
    }
    
    if (lowerMessage.includes('gradient') || lowerMessage.includes('backprop')) {
      return "Backpropagation computes gradients of the loss with respect to each parameter using the chain rule. Gradients indicate how to adjust weights to minimize loss. Vanishing gradients (very small) prevent learning in deep networks - solved by ReLU, skip connections, and batch norm. Exploding gradients are fixed by gradient clipping.";
    }
    
    if (lowerMessage.includes('optimizer') || lowerMessage.includes('sgd') || lowerMessage.includes('adam')) {
      return "Optimizers update weights using gradients. Popular ones: 1) SGD: Simple but effective with momentum, 2) Adam: Adaptive learning rates, works well out-of-box (most popular), 3) RMSprop: Good for RNNs, 4) AdamW: Adam with better weight decay. Learning rate is the most important hyperparameter!";
    }

    if (lowerMessage.includes('learning rate')) {
      return "Learning rate controls how much weights change per update. Too high = unstable training, overshooting. Too low = slow convergence, stuck in local minima. Common strategies: 1) Start with 0.001 (Adam) or 0.1 (SGD), 2) Learning rate decay/scheduling, 3) Warmup, 4) Cyclical learning rates.";
    }
    
    if (lowerMessage.includes('compare') || lowerMessage.includes('difference') || lowerMessage.includes('vs')) {
      return "üìä **Complete Architecture Comparison:**\n\n**AlexNet (2012)** - 60M params, 12 steps\n‚Ä¢ Simple sequential: Conv‚ÜíPool‚ÜíConv‚ÜíPool‚ÜíConv‚ÜíConv‚ÜíConv‚ÜíPool‚ÜíFC‚ÜíFC‚ÜíFC\n‚Ä¢ Large kernels (11√ó11, 5√ó5) ‚Üí small (3√ó3)\n‚Ä¢ Huge FC layers: 102M params in FC6 alone!\n‚Ä¢ Innovation: ReLU, dropout, GPU training\n‚Ä¢ Error: 15.3% top-5\n\n**GoogLeNet (2014)** - 7M params, 22 layers\n‚Ä¢ Inception modules with PARALLEL paths (1√ó1, 3√ó3, 5√ó5, pool)\n‚Ä¢ Most efficient: 20√ó fewer params than VGG!\n‚Ä¢ 1√ó1 convs for dimensionality reduction\n‚Ä¢ Global avg pooling (no huge FC layers)\n‚Ä¢ Error: 6.67% top-5\n\n**VGG-16 (2014)** - 138M params, 22 steps\n‚Ä¢ Uniform architecture: ONLY 3√ó3 convs throughout\n‚Ä¢ 2-2-3-3-3 blocks pattern\n‚Ä¢ 123M params in FC layers (89%!)\n‚Ä¢ Proved: Depth matters, stacked small kernels > large kernels\n‚Ä¢ Most parameters, memory-intensive\n\n**ResNet-50 (2015)** - 25M params, 50 layers\n‚Ä¢ Skip connections: y = F(x) + x\n‚Ä¢ Bottleneck blocks: 1√ó1‚Üí3√ó3‚Üí1√ó1\n‚Ä¢ Solves vanishing gradients ‚Üí enables 50-152 layers\n‚Ä¢ Global avg pooling (no huge FC)\n‚Ä¢ Best accuracy: 3.57% top-5\n‚Ä¢ Innovation: Residual learning is the KEY!\n\n**Efficiency Ranking**: GoogLeNet (7M) > ResNet (25M) < AlexNet (60M) <<< VGG (138M)\n**Accuracy Ranking**: ResNet > GoogLeNet > VGG ‚âà AlexNet";
    }
    
    if (lowerMessage.includes('how many layers') || lowerMessage.includes('layer count') || lowerMessage.includes('depth')) {
      return "üìè **Complete Layer Counts:**\n\n**AlexNet**: 8 weighted layers\n‚Ä¢ 5 convolutional layers (Conv1-5)\n‚Ä¢ 3 fully connected layers (FC6-8)\n‚Ä¢ PLUS 3 max pooling layers\n‚Ä¢ **Total steps in visualizer**: 12\n\n**VGG-16**: 16 weighted layers\n‚Ä¢ 13 convolutional layers (5 blocks: 2-2-3-3-3)\n‚Ä¢ 3 fully connected layers (FC6-8)\n‚Ä¢ PLUS 5 max pooling layers\n‚Ä¢ **Total steps in visualizer**: 22\n\n**GoogLeNet**: 22 weighted layers\n‚Ä¢ 9 inception modules (each has 4 parallel conv paths)\n‚Ä¢ Multiple 1√ó1, 3√ó3, 5√ó5 convolutions\n‚Ä¢ 1 final FC layer (no huge FC layers!)\n‚Ä¢ **Total steps in visualizer**: 17 (representative modules)\n\n**ResNet-50**: 50 weighted layers\n‚Ä¢ 1 initial conv (7√ó7)\n‚Ä¢ 16 bottleneck blocks √ó 3 convs each = 48\n‚Ä¢ 1 final FC layer\n‚Ä¢ Stages: Conv2_x(3), Conv3_x(4), Conv4_x(6), Conv5_x(3)\n‚Ä¢ **Total steps in visualizer**: 17 (representative blocks)\n\nüí° 'Weighted layers' = layers with learnable parameters (conv, FC)\nüí° Pooling, ReLU, BatchNorm don't count toward depth (no params)";
    }
    
    if (lowerMessage.includes('bottleneck') || lowerMessage.includes('1x1 conv') || lowerMessage.includes('1√ó1')) {
      return "üî¨ **Bottleneck Blocks & 1√ó1 Convolutions** - Efficiency Magic!\n\n**What are 1√ó1 convs?**\n‚Ä¢ Operate on EACH pixel independently (no spatial mixing)\n‚Ä¢ Change channel dimensions: 256 channels ‚Üí 64 channels\n‚Ä¢ Add non-linearity (ReLU after each conv)\n‚Ä¢ Extremely cheap: 1√ó1√ó256√ó64 vs 3√ó3√ó256√ó64 (9√ó fewer params!)\n\n**ResNet Bottleneck Block**:\n1. **1√ó1 reduce**: 256‚Üí64 channels (COMPRESS)\n2. **3√ó3 process**: 64‚Üí64 channels (EXTRACT FEATURES)\n3. **1√ó1 expand**: 64‚Üí256 channels (RESTORE)\n4. **Skip**: Add input directly (residual connection)\n\n**Why?** 3√ó3 on 256 channels = expensive\nBottleneck: 1√ó1√ó256√ó64 + 3√ó3√ó64√ó64 + 1√ó1√ó64√ó256 = MUCH CHEAPER!\n\n**GoogLeNet Inception Bottleneck**:\n‚Ä¢ 1√ó1 before 3√ó3 and 5√ó5 paths\n‚Ä¢ Reduces computation by 4-10√ó\n‚Ä¢ Enables deeper networks with fewer parameters\n\n**Network in Network**: 1√ó1 convs were introduced in 2013 (NiN paper)\n‚Üí Popularized by GoogLeNet\n‚Üí Now standard in MobileNet, EfficientNet, etc.\n\nüí° Check Inception modules in Inference Explorer to see parallel 1√ó1 paths!";
    }
    
    if (lowerMessage.includes('skip') || lowerMessage.includes('residual') || lowerMessage.includes('shortcut')) {
      return "üîÑ **Skip Connections / Residual Learning** - ResNet's Breakthrough!\n\n**The Problem** (pre-2015):\n‚Ä¢ Deep networks (>20 layers) performed WORSE than shallow ones\n‚Ä¢ Vanishing gradients: gradients become tiny (10‚Åª¬π‚Å∞)\n‚Ä¢ Network can't learn, even with ReLU and BatchNorm\n\n**The Solution**: y = F(x) + x\n‚Ä¢ Input (x) added DIRECTLY to output\n‚Ä¢ Network learns F(x) = residual (difference)\n‚Ä¢ If layer should do nothing: F(x)=0, output=x (identity)\n\n**Why It Works**:\n1. **Gradient Flow**: Gradients flow directly backward through '+ x'\n2. **Easy Identity**: Learning identity is trivial (set weights to 0)\n3. **Flexibility**: Network chooses when to learn new features\n4. **Depth Enabled**: ResNet-152 works! (vs. VGG-19 max)\n\n**Implementation in ResNet-50**:\n‚Ä¢ Every bottleneck block has skip connection\n‚Ä¢ When dimensions change (downsampling), use 1√ó1 conv on shortcut\n‚Ä¢ Example: 56√ó56√ó256 ‚Üí 28√ó28√ó512\n  - Main path: 1√ó1‚Üí3√ó3(stride=2)‚Üí1√ó1\n  - Skip path: 1√ó1(stride=2) to match dimensions\n\n**Impact**: Enabled 50, 101, 152, even 1000+ layer networks!\n\nüí° Inference Explorer shows skip connections explicitly in ResNet blocks!";
    }
    
    if (lowerMessage.includes('inception module') || lowerMessage.includes('parallel') || lowerMessage.includes('multi-scale')) {
      return "üéØ **Inception Modules** - Multi-Scale Feature Extraction!\n\n**The Idea**: Different filters see different scales!\n‚Ä¢ Small objects need small filters (1√ó1, 3√ó3)\n‚Ä¢ Large objects need large filters (5√ó5)\n‚Ä¢ Solution: Use ALL sizes in PARALLEL!\n\n**Inception Module Structure** (4 parallel paths):\n```\nInput (28√ó28√ó192)\n    ‚Üì\n‚îå‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   ‚îÇ   ‚îÇ    ‚îÇ    ‚îÇ\n1√ó1 1√ó1 1√ó1  3√ó3  ‚Üê Parallel!\n‚îÇ   ‚Üì   ‚Üì   Pool\n‚îÇ  3√ó3 5√ó5   1√ó1\n‚îÇ   ‚îÇ   ‚îÇ    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n        ‚Üì\n   Concatenate (28√ó28√ó256)\n```\n\n**Each Path Captures**:\n1. **1√ó1 path**: Point-wise features (64 filters)\n2. **1√ó1‚Üí3√ó3 path**: Local features (128 filters)\n3. **1√ó1‚Üí5√ó5 path**: Broader features (32 filters)\n4. **Pool‚Üí1√ó1 path**: Max features (32 filters)\n\n**The Magic**: 1√ó1 'reduce' before 3√ó3 and 5√ó5!\n‚Ä¢ Without: 3√ó3√ó192√ó128 = 221K params\n‚Ä¢ With 1√ó1 reduce: (1√ó1√ó192√ó96) + (3√ó3√ó96√ó128) = 128K params\n‚Ä¢ Saves 42% computation!\n\n**Why It Works**:\n‚Ä¢ Network learns which scale to use for each feature\n‚Ä¢ More flexible than single kernel size\n‚Ä¢ Efficient: 7M total params (GoogLeNet)\n\n**In Practice**: GoogLeNet has 9 inception modules\n‚Ä¢ 2 @ 28√ó28 resolution\n‚Ä¢ 5 @ 14√ó14 resolution (most modules)\n‚Ä¢ 2 @ 7√ó7 resolution\n\nüí° Inference Explorer shows inception modules with all 4 paths!";
    }
    
    if (lowerMessage.includes('how') && (lowerMessage.includes('work') || lowerMessage.includes('chatbot') || lowerMessage.includes('answer'))) {
      return "**How I work:** I'm a rule-based chatbot with a curated knowledge base! ü§ñ\n\n1. **Pattern Matching**: I analyze your message for keywords (e.g., 'alexnet', 'convolution', 'dropout')\n2. **Knowledge Base**: I have pre-written responses covering 60+ deep learning topics including:\n   ‚Ä¢ Complete architectures (all layers)\n   ‚Ä¢ Technical innovations (skip connections, inception modules)\n   ‚Ä¢ Parameter counts and efficiency comparisons\n   ‚Ä¢ Historical context and ImageNet results\n3. **Response Selection**: I match your question to the most relevant information\n\n**Updated Knowledge** (Nov 2025):\n‚úÖ All 4 models now show COMPLETE layer-by-layer implementations\n‚úÖ Detailed parameter breakdowns\n‚úÖ Architectural innovations explained\n‚úÖ Inference Explorer integration\n\n**Note**: I'm not a real AI model - I'm a deterministic system designed to help you learn about neural networks! For production, you'd integrate with GPT-4, Claude, or similar LLMs via their APIs.";
    }
    
    if (lowerMessage.includes('inference explorer') || lowerMessage.includes('layer visualization')) {
      return "üîç **Inference Explorer** - See Networks Process Images Layer by Layer!\n\n**What It Does**:\n‚Ä¢ Runs actual image through neural network\n‚Ä¢ Shows EVERY layer's output with visualizations\n‚Ä¢ Displays feature maps (first 16 channels)\n‚Ä¢ Tracks dimensions and parameters\n‚Ä¢ Provides educational explanations\n\n**Complete Implementations**:\n‚úÖ **AlexNet**: 12 steps (all 8 layers + pooling)\n‚úÖ **VGG-16**: 22 steps (all 16 layers + pooling + FC)\n‚úÖ **ResNet-50**: 17 steps (representative bottleneck blocks)\n‚úÖ **GoogLeNet**: 17 steps (representative inception modules)\n\n**How to Use**:\n1. Select a model (AlexNet, VGG, ResNet, GoogLeNet, MobileNet)\n2. Click 'Load Model' (uses pretrained MobileNet weights)\n3. Upload or use sample image\n4. Click 'Run Inference'\n5. Explore each layer:\n   - Feature map visualizations (4√ó4 grid)\n   - Output dimensions (e.g., 56√ó56√ó128)\n   - Parameter counts\n   - Educational explanations\n   - Historical context\n\n**What You'll See**:\n‚Ä¢ Input: 224√ó224√ó3 RGB image\n‚Ä¢ Early layers: Edge detectors (vertical, horizontal, diagonal)\n‚Ä¢ Middle layers: Textures, patterns, shapes\n‚Ä¢ Deep layers: Object parts, holistic features\n‚Ä¢ Output: 1000 class probabilities\n\nüí° **Pro Tip**: Watch how spatial dimensions shrink (224‚Üí112‚Üí56‚Üí28‚Üí14‚Üí7‚Üí1) while channels increase (3‚Üí64‚Üí128‚Üí256‚Üí512‚Üí1000)!";
    }
    
    if (lowerMessage.includes('all layers') || lowerMessage.includes('complete architecture') || lowerMessage.includes('show me')) {
      return "üìã **Want to see all layers?** Try these questions:\n\n**For Complete Breakdowns**:\n‚Ä¢ \"Tell me about AlexNet\" ‚Üí See all 8 layers + specs\n‚Ä¢ \"Tell me about VGG\" ‚Üí See all 16 layers in 5 blocks\n‚Ä¢ \"Tell me about ResNet\" ‚Üí See bottleneck structure\n‚Ä¢ \"Tell me about GoogLeNet\" ‚Üí See inception modules\n\n**For Comparisons**:\n‚Ä¢ \"Compare the models\" ‚Üí Side-by-side comparison\n‚Ä¢ \"How many layers\" ‚Üí Layer counts for all models\n\n**For Technical Details**:\n‚Ä¢ \"Explain bottleneck\" ‚Üí ResNet 1√ó1 convs\n‚Ä¢ \"Explain inception module\" ‚Üí GoogLeNet parallel paths\n‚Ä¢ \"Explain skip connections\" ‚Üí ResNet residual learning\n\n**Best Way to Explore**:\nüéØ Go to **Inference Explorer** page!\n‚Ä¢ Select any model\n‚Ä¢ Run inference on an image\n‚Ä¢ See EVERY layer with:\n  - Feature visualizations\n  - Dimension tracking\n  - Parameter counts\n  - Educational explanations\n  - Historical context\n\nüí° It's like watching the network think!";
    }
    
    if (lowerMessage.includes('parameter') && (lowerMessage.includes('count') || lowerMessage.includes('breakdown') || lowerMessage.includes('distribution'))) {
      return "üìä **Parameter Distribution Breakdown:**\n\n**AlexNet (60M total)**:\n‚Ä¢ Conv1-5: 2.3M (4%)\n‚Ä¢ FC6: 37.7M params (63%) ‚ö†Ô∏è Huge!\n‚Ä¢ FC7: 16.8M params (28%)\n‚Ä¢ FC8: 4.1M params (7%)\n‚Üí FC layers = 97% of all parameters!\n\n**VGG-16 (138M total)**:\n‚Ä¢ Conv1-13: 14.7M (11%)\n‚Ä¢ FC6: 102.8M params (74%) ‚ö†Ô∏è Massive!\n‚Ä¢ FC7: 16.8M params (12%)\n‚Ä¢ FC8: 4.1M params (3%)\n‚Üí FC layers = 89% of all parameters!\n\n**GoogLeNet (7M total)**:\n‚Ä¢ All convs: 6M (86%)\n‚Ä¢ Final FC: 1M (14%)\n‚Üí Global avg pooling eliminates huge FC layers!\n‚Üí Most efficient architecture\n\n**ResNet-50 (25M total)**:\n‚Ä¢ Conv layers: 23M (92%)\n‚Ä¢ Final FC: 2M (8%)\n‚Üí Also uses global avg pooling\n‚Üí No huge FC layers like AlexNet/VGG\n\n**Key Insight**: Modern architectures (GoogLeNet, ResNet) use **Global Average Pooling** instead of huge FC layers:\n‚Ä¢ VGG FC6: 25,088√ó4,096 = 102M params\n‚Ä¢ ResNet GAP: 0 params (just averaging!)\n\n**Evolution**:\n2012 (AlexNet): 97% params in FC ‚Üí Inefficient\n2014 (GoogLeNet): Global pooling ‚Üí 7M total\n2015 (ResNet): Global pooling ‚Üí 25M total\n\nüí° This is why modern networks are more efficient!";
    }
    
    if (lowerMessage.includes('global average pooling') || lowerMessage.includes('gap') || lowerMessage.includes('global pooling')) {
      return "üåê **Global Average Pooling (GAP)** - Modern Efficiency Trick!\n\n**What It Does**:\n‚Ä¢ Takes each feature map (e.g., 7√ó7)\n‚Ä¢ Averages ALL spatial values ‚Üí single number\n‚Ä¢ Example: 7√ó7 feature map ‚Üí 1 value\n‚Ä¢ If 2048 channels: 7√ó7√ó2048 ‚Üí 2048 values\n\n**Why It's Amazing**:\n‚úÖ **Zero parameters!** Just averaging\n‚úÖ Replaces huge FC layers\n‚úÖ Forces convs to learn semantic features\n‚úÖ Reduces overfitting\n‚úÖ Works with any input size\n\n**Comparison**:\n**VGG-16 (no GAP)**:\n‚Ä¢ 7√ó7√ó512 ‚Üí Flatten ‚Üí 25,088 values\n‚Ä¢ FC6: 25,088 √ó 4,096 = 102M params üò±\n\n**ResNet-50 (with GAP)**:\n‚Ä¢ 7√ó7√ó2048 ‚Üí GAP ‚Üí 2,048 values\n‚Ä¢ FC: 2,048 √ó 1,000 = 2M params ‚úÖ\n‚Ä¢ **Savings**: 100M parameters!\n\n**When Introduced**:\n‚Ä¢ Network in Network (NiN) paper, 2013\n‚Ä¢ Popularized by GoogLeNet, 2014\n‚Ä¢ Standard in ResNet (2015) and beyond\n\n**How It Works**:\n```python\n# Input: 7√ó7√ó2048 feature maps\nfor each of 2048 channels:\n    value = average(7√ó7 spatial positions)\n# Output: 2048 values ‚Üí FC layer\n```\n\n**Modern Usage**:\n‚Ä¢ ResNet: 7√ó7√ó2048 ‚Üí GAP ‚Üí 2048\n‚Ä¢ MobileNet: 7√ó7√ó1024 ‚Üí GAP ‚Üí 1024\n‚Ä¢ EfficientNet: Variable ‚Üí GAP ‚Üí channels\n\nüí° GAP is why ResNet is 5√ó more parameter-efficient than VGG!";
    }
    
    if (lowerMessage.includes('skip connection') || lowerMessage.includes('residual')) {
      return "Skip connections (residual connections) add the input of a block directly to its output: y = F(x) + x. This allows gradients to flow directly backward, preventing vanishing gradients. Benefits: 1) Train much deeper networks (100+ layers), 2) Better gradient flow, 3) Learn identity mappings easily, 4) Improved accuracy. ResNet popularized this technique!";
    }
    
    if (lowerMessage.includes('data augmentation')) {
      return "Data augmentation artificially increases training data by applying transformations: 1) Geometric (rotation, flipping, cropping, scaling), 2) Color (brightness, contrast, saturation), 3) Noise injection, 4) Cutout/Mixup. Benefits: Reduces overfitting, improves generalization, makes models robust to variations. Essential for image classification!";
    }
    
    if (lowerMessage.includes('feature map') || lowerMessage.includes('channel')) {
      return "A feature map (or channel) is the output of applying one filter to an input. For example, a conv layer with 64 filters produces 64 feature maps. Each feature map detects specific patterns - early layers: edges/colors, middle layers: textures/parts, deep layers: objects/concepts. The depth (number of channels) represents representational capacity.";
    }
    
    if (lowerMessage.includes('inception') && lowerMessage.includes('module')) {
      return "An Inception module applies multiple filter sizes (1√ó1, 3√ó3, 5√ó5) in parallel, then concatenates results. This captures features at multiple scales simultaneously! The 1√ó1 convolutions before larger filters reduce computational cost (dimensionality reduction). It's like having multiple experts looking at the same data from different perspectives.";
    }
    
    if (lowerMessage.includes('dashboard') || lowerMessage.includes('calculator') || lowerMessage.includes('theory')) {
      return "The Dashboard has 4 powerful tabs:\n\n**1Ô∏è‚É£ Theory Tab**: Learn about Conv, Pooling, FC, and Dropout layers with formulas, properties, and numerical examples.\n\n**2Ô∏è‚É£ Calculator Tab**: Interactive tool to calculate output dimensions, parameters, FLOPs, and memory usage for convolutional layers. Try different inputs!\n\n**3Ô∏è‚É£ Examples Tab**: Real-world layer configurations from AlexNet, VGG, ResNet, and MobileNet. Click 'Try in Calculator' to experiment!\n\n**4Ô∏è‚É£ Data Transformation**: Visual guide showing how data flows through Conv, ReLU, BatchNorm, Pooling, Dropout, and FC layers with actual values!";
    }
    
    if (lowerMessage.includes('transformation') || lowerMessage.includes('data flow') || lowerMessage.includes('how data')) {
      return "**Data Transformation in Neural Networks**:\n\n1Ô∏è‚É£ **Input**: Raw pixel values (e.g., 5√ó5 image)\n2Ô∏è‚É£ **Convolution**: Extract features using kernels (3√ó3√ó2 filters)\n3Ô∏è‚É£ **ReLU**: Apply activation, zero out negatives\n4Ô∏è‚É£ **Batch Norm**: Normalize to mean=0, std=1 for stability\n5Ô∏è‚É£ **Pooling**: Downsample (e.g., 4√ó4 ‚Üí 2√ó2 max pool)\n6Ô∏è‚É£ **Dropout**: Randomly deactivate neurons (training only)\n7Ô∏è‚É£ **Flatten**: Convert 2D to 1D vector\n8Ô∏è‚É£ **FC**: Fully connected classification\n\nCheck the Dashboard's Data Transformation tab to see this with actual values!";
    }
    
    if (lowerMessage.includes('softmax') || lowerMessage.includes('output layer')) {
      return "**Softmax Activation** converts raw scores (logits) into probabilities that sum to 1.0:\n\nFormula: softmax(xi) = exp(xi) / Œ£exp(xj)\n\nExample:\n‚Ä¢ Input: [2.0, 1.0, 0.1]\n‚Ä¢ After exp: [7.39, 2.72, 1.11]\n‚Ä¢ Sum: 11.22\n‚Ä¢ Softmax: [0.659, 0.242, 0.099] ‚Üê Probabilities!\n\nUsed in final classification layer. The highest probability indicates the predicted class. Often paired with cross-entropy loss during training.";
    }
    
    if (lowerMessage.includes('flops') || lowerMessage.includes('computation') || lowerMessage.includes('efficiency')) {
      return "**FLOPs (Floating Point Operations)** measure computational cost:\n\nFor Conv Layer:\nFLOPs = Output_H √ó Output_W √ó Kernel_H √ó Kernel_W √ó Input_Channels √ó Output_Channels\n\n**Example**: Conv layer with 3√ó3 kernel, 64 input channels, 128 output channels, 56√ó56 output:\nFLOPs = 56 √ó 56 √ó 3 √ó 3 √ó 64 √ó 128 = 231M FLOPs\n\n**Why it matters**:\n‚Ä¢ Mobile devices: Need low FLOPs (<100M)\n‚Ä¢ Edge devices: Target <1B FLOPs\n‚Ä¢ Cloud servers: Can handle 10B+ FLOPs\n\nUse the Calculator tab to compute FLOPs for your layers!";
    }
    
    if (lowerMessage.includes('memory') || lowerMessage.includes('gpu') || lowerMessage.includes('vram')) {
      return "**Memory Usage in Neural Networks**:\n\n**1. Parameters (Weights)**:\n‚Ä¢ Conv: kernel_h √ó kernel_w √ó in_ch √ó out_ch √ó 4 bytes\n‚Ä¢ FC: input_size √ó output_size √ó 4 bytes\n\n**2. Activations (Forward Pass)**:\n‚Ä¢ Store all layer outputs for backprop\n‚Ä¢ batch_size √ó height √ó width √ó channels √ó 4 bytes\n\n**3. Gradients (Backward Pass)**:\n‚Ä¢ Same size as activations\n‚Ä¢ Temporary during training\n\n**Tips to reduce memory**:\n‚Ä¢ Smaller batch size\n‚Ä¢ Gradient checkpointing\n‚Ä¢ Mixed precision (FP16)\n‚Ä¢ Prune unnecessary layers\n\nCalculator tab shows parameter memory!";
    }
    
    if (lowerMessage.includes('stride') || lowerMessage.includes('padding')) {
      return "**Stride & Padding** control output dimensions:\n\n**Stride**: Step size when sliding filter\n‚Ä¢ Stride=1: Dense sampling, larger output\n‚Ä¢ Stride=2: Skip every other position, 2√ó smaller output\n‚Ä¢ Higher stride = faster but less detail\n\n**Padding**: Border pixels added\n‚Ä¢ Valid (no padding): Output shrinks\n‚Ä¢ Same (pad to maintain size): Output = Input\n‚Ä¢ Formula: pad = (kernel_size - 1) / 2\n\n**Output Size Formula**:\nOutput = ‚åä(Input + 2√óPad - Kernel) / Stride‚åã + 1\n\nExample: 32√ó32 input, 5√ó5 kernel, stride=1, pad=2\n‚Üí ‚åä(32 + 4 - 5) / 1‚åã + 1 = 32√ó32 output\n\nTry the Calculator tab!";
    }
    
    if (lowerMessage.includes('visualizer') || lowerMessage.includes('graph') || lowerMessage.includes('network view')) {
      return "**Network Visualizer Page Features**:\n\nüé® **Interactive Graph**:\n‚Ä¢ Visual representation of model architecture\n‚Ä¢ Zoom, pan, and navigate freely\n‚Ä¢ Click any layer node to see detailed specs\n\nüìä **Layer Details** (click a node):\n‚Ä¢ Type, input/output dimensions\n‚Ä¢ Kernel size, stride, padding\n‚Ä¢ Number of parameters\n‚Ä¢ Activation functions\n\nüîç **Special Features**:\n‚Ä¢ ResNet shows skip connections with branching paths\n‚Ä¢ Edge labels show stride information\n‚Ä¢ Animated connections between layers\n‚Ä¢ MiniMap for navigation\n\nü§ñ **Models Available**:\n‚Ä¢ AlexNet, ResNet-50, GoogLeNet, VGG-16\n\nExplore the Visualizer page from the top navigation!";
    }
    
    if (lowerMessage.includes('calculator') && lowerMessage.includes('use')) {
      return "**How to Use the Calculator Tab**:\n\n1Ô∏è‚É£ **Enter Input Dimensions**:\n‚Ä¢ Height, Width, Channels (e.g., 224√ó224√ó3 for RGB image)\n\n2Ô∏è‚É£ **Set Kernel Parameters**:\n‚Ä¢ Kernel Size (e.g., 3√ó3, 5√ó5, 7√ó7)\n‚Ä¢ Stride (typically 1 or 2)\n‚Ä¢ Padding (0 for valid, auto for same)\n‚Ä¢ Number of Filters (output channels)\n\n3Ô∏è‚É£ **View Results**:\n‚Ä¢ ‚úÖ Output Dimensions\n‚Ä¢ üìä Total Parameters\n‚Ä¢ ‚ö° FLOPs (computational cost)\n‚Ä¢ üíæ Memory Usage\n\nüí° **Pro Tip**: Try the example configurations from the Examples tab to see real-world settings from famous architectures!";
    }
    
    // Default responses with helpful suggestions
    const defaultResponses = [
      "I'd be happy to help! Try asking about:\n‚Ä¢ **Complete Architectures**: 'Tell me about VGG-16', 'Show AlexNet layers'\n‚Ä¢ **Innovations**: 'Explain skip connections', 'What are inception modules?'\n‚Ä¢ **Comparisons**: 'Compare ResNet vs VGG', 'Parameter breakdown'\n‚Ä¢ **Technical**: 'How many layers in ResNet?', 'What is bottleneck design?'\n‚Ä¢ **Tools**: 'How to use Inference Explorer?'",
      `Currently viewing: **${currentModel}**. Ask me:\n‚Ä¢ 'What makes ${currentModel} special?'\n‚Ä¢ 'Show me all ${currentModel} layers'\n‚Ä¢ 'How does it compare to other models?'\n‚Ä¢ 'What innovations does it have?'\n‚Ä¢ 'Parameter breakdown for ${currentModel}'`,
      "üí° **Popular questions about complete architectures:**\n‚Ä¢ 'Tell me about AlexNet' ‚Üí All 8 layers breakdown\n‚Ä¢ 'Tell me about VGG-16' ‚Üí All 16 layers in 5 blocks\n‚Ä¢ 'Tell me about ResNet-50' ‚Üí Bottleneck blocks explained\n‚Ä¢ 'Tell me about GoogLeNet' ‚Üí Inception modules\n‚Ä¢ 'Compare the models' ‚Üí Side-by-side analysis",
      "üéØ **New! Complete Architecture Knowledge:**\n‚úì All layers for AlexNet, VGG, ResNet, GoogLeNet\n‚úì Parameter counts and distribution\n‚úì Skip connections & inception modules\n‚úì Inference Explorer integration\n‚úì Historical context & innovations\n\nWhat would you like to explore?",
      "üìö **I can help with:**\n‚Ä¢ **Models**: Complete layer breakdowns (AlexNet, VGG, ResNet, GoogLeNet)\n‚Ä¢ **Layers**: Conv, Pooling, FC, Dropout, BatchNorm, Activations\n‚Ä¢ **Innovations**: Skip connections, Inception modules, Bottlenecks, GAP\n‚Ä¢ **Training**: Optimizers, Learning Rate, Regularization\n‚Ä¢ **Comparisons**: Parameters, efficiency, accuracy\n\nAsk me anything!",
    ];
    
    return defaultResponses[Math.floor(Math.random() * defaultResponses.length)];
  };

  const handleSend = () => {
    if (!input.trim()) return;

    const userMessage: ChatMessage = {
      id: Date.now().toString(),
      role: 'user',
      content: input,
      timestamp: new Date(),
    };

    setMessages((prev) => [...prev, userMessage]);
    setInput('');

    // Simulate AI response with delay
    setTimeout(() => {
      const aiMessage: ChatMessage = {
        id: (Date.now() + 1).toString(),
        role: 'assistant',
        content: getAIResponse(input),
        timestamp: new Date(),
      };
      setMessages((prev) => [...prev, aiMessage]);
    }, 500);
  };

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSend();
    }
  };

  if (isMinimized) {
    return (
      <div className="fixed bottom-6 right-6 z-40">
        <button
          onClick={() => setIsMinimized(false)}
          className="bg-gradient-to-r from-blue-600 to-indigo-600 text-white p-4 rounded-full shadow-lg hover:shadow-xl transition-all flex items-center gap-2"
        >
          <MessageSquare className="w-6 h-6" />
          <span className="font-medium">Chat Assistant</span>
        </button>
      </div>
    );
  }

  return (
    <div
      className={`fixed ${
        isExpanded ? 'inset-4' : 'bottom-6 right-6 w-96 h-[600px]'
      } bg-white rounded-2xl shadow-2xl z-40 flex flex-col border border-gray-200 transition-all duration-300`}
    >
      {/* Header */}
      <div className="bg-gradient-to-r from-blue-600 to-indigo-600 text-white p-4 rounded-t-2xl flex items-center justify-between">
        <div className="flex items-center gap-3">
          <div className="bg-white bg-opacity-20 p-2 rounded-lg">
            <Bot className="w-6 h-6" />
          </div>
          <div>
            <h3 className="font-bold text-lg">Neural Network Assistant</h3>
            <p className="text-sm opacity-90">Ask me anything!</p>
          </div>
        </div>
        <div className="flex gap-2">
          <button
            onClick={() => setIsExpanded(!isExpanded)}
            className="p-2 hover:bg-white hover:bg-opacity-20 rounded-lg transition-colors"
          >
            {isExpanded ? <Minimize2 className="w-5 h-5" /> : <Maximize2 className="w-5 h-5" />}
          </button>
          <button
            onClick={() => setIsMinimized(true)}
            className="p-2 hover:bg-white hover:bg-opacity-20 rounded-lg transition-colors"
          >
            <X className="w-5 h-5" />
          </button>
        </div>
      </div>

      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4 space-y-4">
        {messages.map((message) => (
          <div
            key={message.id}
            className={`flex gap-3 ${message.role === 'user' ? 'flex-row-reverse' : 'flex-row'}`}
          >
            <div
              className={`flex-shrink-0 w-8 h-8 rounded-full flex items-center justify-center ${
                message.role === 'user' ? 'bg-blue-600' : 'bg-gradient-to-br from-indigo-500 to-purple-500'
              }`}
            >
              {message.role === 'user' ? (
                <User className="w-5 h-5 text-white" />
              ) : (
                <Bot className="w-5 h-5 text-white" />
              )}
            </div>
            <div
              className={`flex-1 px-4 py-3 rounded-2xl ${
                message.role === 'user'
                  ? 'bg-blue-600 text-white'
                  : 'bg-gray-100 text-gray-800'
              }`}
            >
              <div className="text-sm leading-relaxed whitespace-pre-wrap">
                {message.content.split('\n').map((line, i) => {
                  // Simple markdown-like formatting
                  if (line.startsWith('**') && line.includes(':**')) {
                    const parts = line.split(':**');
                    return (
                      <p key={i} className="font-bold mb-2 text-blue-700">
                        {parts[0].replace(/\*\*/g, '')}:
                      </p>
                    );
                  } else if (line.startsWith('**') && line.endsWith('**')) {
                    return (
                      <p key={i} className="font-bold mb-1">
                        {line.replace(/\*\*/g, '')}
                      </p>
                    );
                  } else if (line.startsWith('‚Ä¢') || line.startsWith('‚úì')) {
                    return (
                      <p key={i} className="ml-2 mb-1">
                        {line}
                      </p>
                    );
                  } else if (line.trim() === '') {
                    return <br key={i} />;
                  } else {
                    return <p key={i} className="mb-1">{line}</p>;
                  }
                })}
              </div>
              <p
                className={`text-xs mt-2 ${
                  message.role === 'user' ? 'text-blue-100' : 'text-gray-500'
                }`}
              >
                {message.timestamp.toLocaleTimeString()}
              </p>
            </div>
          </div>
        ))}
        <div ref={messagesEndRef} />
      </div>

      {/* Quick Questions (show when messages are minimal) */}
      {messages.length <= 1 && (
        <div className="px-4 pb-2">
          <p className="text-xs text-gray-500 mb-2">Quick questions:</p>
          <div className="flex flex-wrap gap-2">
            {[
              'What is the dashboard?',
              'How does data transformation work?',
              'Compare the models',
              'Explain batch normalization',
              'How to use calculator?',
              'What is dropout?',
            ].map((question, idx) => (
              <button
                key={idx}
                onClick={() => {
                  setInput(question);
                  setTimeout(() => handleSend(), 100);
                }}
                className="text-xs px-3 py-1.5 bg-blue-50 text-blue-600 rounded-full hover:bg-blue-100 transition-colors"
              >
                {question}
              </button>
            ))}
          </div>
        </div>
      )}

      {/* Input */}
      <div className="p-4 border-t border-gray-200">
        <div className="flex gap-2">
          <input
            type="text"
            value={input}
            onChange={(e) => setInput(e.target.value)}
            onKeyPress={handleKeyPress}
            placeholder="Ask about neural networks..."
            className="flex-1 px-4 py-3 border border-gray-300 rounded-xl focus:outline-none focus:ring-2 focus:ring-blue-500 focus:border-transparent"
          />
          <button
            onClick={handleSend}
            disabled={!input.trim()}
            className="bg-blue-600 text-white p-3 rounded-xl hover:bg-blue-700 disabled:bg-gray-300 disabled:cursor-not-allowed transition-colors"
          >
            <Send className="w-5 h-5" />
          </button>
        </div>
        <p className="text-xs text-gray-400 mt-2 text-center">
          üí° Rule-based chatbot with pre-programmed knowledge
        </p>
      </div>
    </div>
  );
};

export default Chatbot;
